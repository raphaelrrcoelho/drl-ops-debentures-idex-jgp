# ========================
# Training Configuration
# ========================

# PPO Hyperparameters
policy: MultiInputPolicy
total_timesteps: 40960  # Increased to be meaningful (multiple of n_steps)
learning_rate: 5.0e-6
n_steps: 256       
batch_size: 124
n_epochs: 10
gamma: 0.99
gae_lambda: 0.95
clip_range: 0.075
clip_range_vf: 0.075
ent_coef: 0.0003
vf_coef: 0.5
max_grad_norm: 0.3
target_kl: 0.125
net_arch: [256, 256]
ortho_init: true
activation: tanh

# ========================
# Environment Configuration
# ========================

# Portfolio constraints
rebalance_interval: 5
max_weight: 0.10
weight_blocks: 100  # For discrete action space (1% granularity)
allow_cash: true
cash_rate_as_rf: true
on_inactive: to_cash  # or "pro_rata"

# Transaction costs
transaction_cost_bps: 20.0
delist_extra_bps: 20.0  # Extra slippage on forced liquidation

# Observation processing
normalize_features: true
obs_clip: 7.5
include_prev_weights: false
include_active_flag: false
global_stats: true
use_momentum_features: true
use_volatility_features: false
use_relative_value_features: true
use_duration_features: true
use_microstructure_features: false
use_carry_features: true
use_spread_dynamics: true
use_risk_adjusted_features: false
use_sector_curves: true
use_zscore_features: false
use_rolling_zscores: false

# Reward configuration
weight_alpha: 1.0   # Weight for alpha (r - index)

# Reward penalties
lambda_turnover: 0.0002
lambda_hhi: 0.01
lambda_drawdown: 0.005
lambda_tail: 0.001  # Set to 0 to disable tail risk penalty

# Tail risk configuration (only used if lambda_tail > 0)
tail_window: 60
tail_q: 0.05
dd_mode: incremental  # or "level"

# Episode control
max_steps: 256  # Episode length matches steps per env per rollout
random_reset_frac: 0.9  # For training data augmentation
max_assets: 50  # Use only top 50 assets by weight each day

# ========================
# Validation Configuration
# ========================

# In-sample parameter validation
do_validation: false  # Set to false to skip validation
validation_timesteps: 1000  # Timesteps for inner validation loop
validation_split: 0.8  # Train/val split for parameter selection
selection_metric: ir  # Metric to optimize: "ir", "sharpe", "sortino", "calmar"

# Parameter grid for validation (set to null to skip)
lambda_grid: null  # Disabled for faster training

# ========================
# Training Structure
# ========================

# Walk-forward configuration
n_folds: 9
embargo_days: 3
seeds: "0,1,2,4"  # Comma-separated string

# Checkpoint and logging
checkpoint_freq: 500000
tensorboard_log: true
verbose: 0  # 0=silent, 1=info, 2=debug

# Parallel training
n_envs: 16  # Number of parallel environments per training
n_jobs: 1  # Number of parallel fold/seed combinations

# Training options
skip_finished: true
resume_from_checkpoint: true
episode_len: 256  # Set to match max_steps for consistency
reset_jitter_frac: 0.9  # Randomize starting point in training data
vec: subproc  # Use subprocess vectorization for parallel envs

# ========================
# Evaluation Configuration
# ========================

evaluation:
  deterministic: true  # Use deterministic policy
  save_weights: true  # Save portfolio weights
  save_topk: true  # Save top-k holdings
  topk_k: 5  # Number of top holdings to track
  
# ========================
# Baseline Configuration
# ========================

baselines:
  strategies: "EW,INDEX,RP_VOL,RP_DURATION,CARRY_TILT,MINVAR"
  roll_window: 60  # Lookback for risk calculations
  ridge: 1.0e-4  # Ridge parameter for MINVAR

# =======================
# Analysis Configuration
# ========================

analysis:
  # Bootstrap confidence intervals
  n_bootstrap: 2000
  block_size: 21
  
  # Sensitivity analysis
  run_sensitivity: true
  sensitivity_params: "lambda_turnover,lambda_hhi,lambda_drawdown,transaction_cost_bps"
  sensitivity_values: "0.5,1.0,2.0"  # Multiplicative factors
  
  # Statistical tests
  dm_pairs: "PPO:EW,PPO:INDEX,PPO:MINVAR,PPO:RP_VOL"
  var_alpha: 0.95
  nw_lag: null  # null for automatic